{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.decomposition import IncrementalPCA, PCA, TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.feature_extraction.text as extraction\n",
    "import sklearn\n",
    "import scipy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shortX = pd.read_csv(\"./imdbmovies/features.csv\")\n",
    "fullX = pd.read_csv(\"./imdbmovies/features_vectorized.csv\")\n",
    "words50X = pd.read_csv(\"./imdbmovies/vectorization50.csv\", header=None)\n",
    "labelsY = pd.read_csv(\"./imdbmovies/labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing:\n",
    "- removing all data whos feature vector is irregular (longer than the column of 44 or something)\n",
    "- removed all commas in CSV so that it doesn't interfere with the read_csv\n",
    "- drop features with missing values\n",
    "- deleted non-movies\n",
    "- threw out data with multiple parentheses appearences\n",
    "- translate all titles to english\n",
    "- removed year and other parentheses from title\n",
    "- choosing to use snowball(Porter2) stemmer\n",
    "- remove punctuation\n",
    "- parsed titles and created tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This requires google cloud api credential which is not attached, the following function cannot be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    client = translate.Client()\n",
    "    stemmer = stem.snowball.EnglishStemmer()    \n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    data = pd.read_csv(\"./imdbmovies/imdb.csv\")\n",
    "    \n",
    "    idxRemove = data[np.logical_not(np.isnan(data['Unnamed: 44']))].index\n",
    "    dataCleaned = data.drop(idxRemove)\n",
    "    idxNonMovie = dataCleaned[dataCleaned[\"type\"] != 'video.movie'].index\n",
    "    dataCleaned.drop(idxNonMovie, inplace=True)\n",
    "    idxTwoParen = dataCleaned[dataCleaned[\"title\"].str.contains(\"\\(.*\\(\")].index\n",
    "    dataCleaned.drop(idxTwoParen, inplace=True)\n",
    "\n",
    "    dataCleaned.drop(columns=[\"Unnamed: 44\", \"Unnamed: 45\", \"Unnamed: 46\", \"Unnamed: 47\"], inplace=True)\n",
    "    dataCleaned.dropna(axis=0, inplace=True)\n",
    "    dataCleaned.reset_index(inplace=True)\n",
    "    dataCleaned.drop(columns=[\"index\", \"url\", \"tid\", \"fn\", \"wordsInTitle\", \"type\"], inplace=True)\n",
    "    dataCleaned.title = dataCleaned.title.apply(lambda title: re.sub(r'\\(([ a-zA-Z]*)([0-9]{4})\\)', r'\\1', title))\\\n",
    "                                    .apply(lambda title: title.translate(table))\\\n",
    "                                    .apply(lambda title: client.translate(title)[\"translatedText\"].replace(\"&#39;\", \"\"))\\\n",
    "                                    .apply(lambda title: \" \".join([stemmer.stem(word) for word in tokenize(title)]))\n",
    "    \n",
    "    X = dataCleaned.iloc[:, :11]\n",
    "    y = dataCleaned.iloc[:, 11:]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0f5c96aae2f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtfidfVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfVector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshortX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfidfVector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "tfidfVector = extraction.TfidfVectorizer()\n",
    "tfidf = tfidfVector.fit_transform(shortX['title'])\n",
    "key = {v: k for k,v in tfidfVector.vocabulary_.items()}\n",
    "words = pd.DataFrame(tfidf.todense())\n",
    "words.rename(columns=key, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud\n",
    "\n",
    "World clouds based on Genre (Drama, Romance and Comedy) which are genres with more frequency of occurence in the current dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on Drama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_cloud=pd.concat([shortX['title'], labelsY['Drama']], axis=1)\n",
    "movies_for_wordcloud = word_cloud[word_cloud.Drama != 0]\n",
    "values = \" \".join(map(str, (word_cloud[\"title\"].tolist())))\n",
    "WordCloud(stopwords=STOPWORDS,width=1500,height=500).generate(values).to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on Comedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud=pd.concat([shortX['title'], labelsY['Comedy']], axis=1)\n",
    "movies_for_wordcloud = word_cloud[word_cloud.Comedy != 0]\n",
    "values = \" \".join(map(str, (word_cloud[\"title\"].tolist())))\n",
    "WordCloud(stopwords=STOPWORDS,width=1500,height=500).generate(values).to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on Romance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_cloud=pd.concat([shortX['title'], labelsY['Romance']], axis=1)\n",
    "movies_for_wordcloud = word_cloud[word_cloud.Romance != 0]\n",
    "values = \" \".join(map(str, (word_cloud[\"title\"].tolist())))\n",
    "WordCloud(stopwords=STOPWORDS,width=1500,height=500).generate(values).to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genre Fequency\n",
    "\n",
    "Plotting a bar graph based which shows genre vs their frequency of occurence in the current dataset\n",
    "\n",
    "Frequency Caluclation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsY.columns\n",
    "frequency = {}\n",
    "for i in labelsY.columns.values:\n",
    "    frequency[i] = sum(labelsY[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12,10]\n",
    "df=pd.DataFrame.from_dict(frequency, orient=\"index\")\n",
    "df.plot(kind='bar',  ylim=(10,5050), legend = False, title='Frequencies for each Genre')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del shortX['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression without PCA\n",
    "\n",
    "C=np.power(np.e, np.random.uniform(0, 1, 10))\n",
    "start_time = time.time()\n",
    "dict_loss = {}\n",
    "dict_accuracy = {}\n",
    "for c in C:\n",
    "    dict_loss[c] = []\n",
    "    dict_accuracy[c] = []\n",
    "    for i in range (5):\n",
    "        x_train, x_test, y_train, y_test  = train_test_split(shortX, labelsY, test_size=.3)\n",
    "        lr = OneVsRestClassifier(LogisticRegression(class_weight='balanced', C=c, solver='sag', max_iter = 2500), n_jobs=-1)\n",
    "        lr.fit(x_train, y_train)\n",
    "        y_pred = lr.predict(x_test)\n",
    "        score=lr.score(x_test, y_test)\n",
    "        hl = hamming_loss(y_test, y_pred)\n",
    "        dict_loss[c].append(hl)\n",
    "        dict_accuracy[c].append(score)\n",
    "        \n",
    "print(\"Time to load data: {} seconds\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# Plotting the accuracy and hamming loss vs C values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24,12))\n",
    "ax2 = ax.twinx()\n",
    "for c, x in dict_loss.items(): \n",
    "    avg_loss = {k:np.mean(np.array(v)) for k,v in dict_loss.items()}\n",
    "    avg_score = {k:np.mean(np.array(v)) for k,v in dict_accuracy.items()}\n",
    "    list1 = sorted(avg_loss.items())\n",
    "    list2 = sorted(avg_score.items())\n",
    "    x_plot, y_plot = zip(*list1)\n",
    "    ax.plot(x_plot, y_plot, color='orange',marker='d',markersize=10)\n",
    "    x, y = zip(*list2)\n",
    "    ax2.plot(x,y, color='green',marker='d',markersize=10)\n",
    "\n",
    "ax.set_ylabel('Accuracy',fontsize=10)\n",
    "ax2.set_ylabel('Hamming Loss',fontsize=10)\n",
    "ax.legend(['Accuracy'],loc=2,fontsize=15)\n",
    "ax2.legend(['Loss'],loc=1,fontsize=15)\n",
    "ax.set_xlabel('C- values',fontsize=10)\n",
    "plt.title(\"Accuracy and Hamming Loss vs C- values without PCA\")\n",
    "ax.set_xscale('log')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Logistic Regression with PCA\n",
    "\n",
    "start_time = time.time()\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "dict_loss_pca = {}\n",
    "dict_accuracy_pca = {}\n",
    "for c in C:\n",
    "    dict_loss_pca[c] = []\n",
    "    dict_accuracy_pca[c] = []\n",
    "    for i in range (5):\n",
    "        x_train, x_test, y_train, y_test  = train_test_split(shortX, labelsY, test_size=.3)\n",
    "        pca.fit(x_train)\n",
    "        x_train_reduced = pca.transform((x_train))\n",
    "        x_test_reduced = pca.transform((x_test))\n",
    "        lr_pca = OneVsRestClassifier(LogisticRegression(class_weight='balanced', C=c, solver='sag', max_iter = 5000), n_jobs=-1)\n",
    "        lr_pca.fit(x_train_reduced, y_train)\n",
    "        y_pred = lr_pca.predict(x_test_reduced)\n",
    "        score=lr_pca.score(x_test_reduced, y_test)\n",
    "        hl = hamming_loss(y_test, y_pred)\n",
    "        dict_loss_pca[c].append(hl)\n",
    "        dict_accuracy_pca[c].append(score)\n",
    "\n",
    "\n",
    "print(\"Time to load data: {} seconds\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# Plotting LR with PCA\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24,12))\n",
    "ax2 = ax.twinx()\n",
    "for c, x in dict_loss.items(): \n",
    "    avg_loss = {k:np.mean(np.array(v)) for k,v in dict_loss_pca.items()}\n",
    "    avg_score = {k:np.mean(np.array(v)) for k,v in dict_accuracy_pca.items()}\n",
    "    list1 = sorted(avg_loss.items())\n",
    "    list2 = sorted(avg_score.items())\n",
    "    x_plot, y_plot = zip(*list1)\n",
    "    ax.plot(x_plot, y_plot, color='orange',marker='d',markersize=10)\n",
    "    x, y = zip(*list2)\n",
    "    ax2.plot(x,y, color='green',marker='d',markersize=10)\n",
    "\n",
    "ax.set_ylabel('Accuracy',fontsize=10)\n",
    "ax2.set_ylabel('Hamming Loss',fontsize=10)\n",
    "ax.legend(['Accuracy'],loc=2,fontsize=15)\n",
    "ax2.legend(['Loss'],loc=1,fontsize=15)\n",
    "ax.set_xlabel('C- values',fontsize=10)\n",
    "plt.title(\"Accuracy and Hamming Loss vs C- values with PCA\")\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a prediction matrix, if all predicted values are < 0, mark the maximum value as the predicted class and if there are several values > 0 mark all of those value as the predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(arr):\n",
    "    if arr[arr  > 0].size == 0:\n",
    "        result = np.zeros(arr.shape)\n",
    "        maxIdx = np.argmax(arr)\n",
    "        result[maxIdx] = 1\n",
    "        return result\n",
    "    else:\n",
    "        result = arr\n",
    "        result[result > 0] = 1\n",
    "        result[result <= 0] = 0\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titleKey = {0: \"No Title Data\", 1: \"50% Covar Title Data\", 2: \"Full Title Data\"}\n",
    "Loss = {v: [] for k, v in titleKey.items()}\n",
    "Score = {v: [] for k, v in titleKey.items()}\n",
    "for t in range(15):\n",
    "    for i, X in enumerate((shortX, shortX.join(words50X), fullX)):\n",
    "        title = titleKey[i]\n",
    "        X_train, X_test, y_train, y_test  = train_test_split(X, labelsY, test_size=.1)\n",
    "        SVM = LinearSVC(dual=False, max_iter=10000)\n",
    "        clf = OneVsRestClassifier(SVM)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.decision_function(X_test)\n",
    "        y_pred = np.apply_along_axis(get_pred, 1, y_pred)\n",
    "        ham_loss = hamming_loss(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        Score[title].append(acc)\n",
    "        Loss[title].append(ham_loss)\n",
    "        \n",
    "plt.clf()\n",
    "_, ax = plt.subplots(figsize=(24,12))\n",
    "for k, v in trialScore.items():\n",
    "    ax.plot(v, label=k)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.title(\"Accuracy Of Datasets\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "_, ax = plt.subplots()\n",
    "for k, v in trialLoss.items(figsize=(24,12)):\n",
    "    ax.plot(v, label=k)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.title(\"Hamming Loss Of Datasets\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = fullX.to_numpy()\n",
    "labels = labelsY.to_numpy()\n",
    "# k-fold crossvalidation with 10 folds\n",
    "kf = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# calculate accuracy and loss\n",
    "result = np.zeros(10)\n",
    "loss = np.zeros(10)\n",
    "genre_neigh = KNeighborsClassifier(n_neighbors=13)\n",
    "i = 0\n",
    "for train_idx, test_idx in kf.split(features):\n",
    "    X_train2, X_test2 = features[train_idx], features[test_idx]\n",
    "    y_train2, y_test2 = labels[train_idx], labels[test_idx]\n",
    "    genre_neigh.fit(X_train2, y_train2)\n",
    "    y_predict2 = genre_neigh.predict(X_test2)\n",
    "    result[i] = genre_neigh.score(X_test2, y_test2)\n",
    "    loss[i] = hamming_loss(y_test2, y_predict2)\n",
    "    i += 1\n",
    "\n",
    "words50 = words50X.to_numpy()    \n",
    "# calculate accuracy and loss with 50% covar, 10 fold CV\n",
    "result_50 = np.zeros(10)\n",
    "loss_50 = np.zeros(10)\n",
    "i = 0\n",
    "for train_idx, test_idx in kf.split(words50):\n",
    "    X_train3, X_test3 = words50[train_idx], words50[test_idx]\n",
    "    y_train3, y_test3 = labels[train_idx], labels[test_idx]\n",
    "    genre_neigh.fit(X_train3, y_train3)\n",
    "    y_predict3 = genre_neigh.predict(X_test3)\n",
    "    result_50[i] = genre_neigh.score(X_test3, y_test3)\n",
    "    loss_50[i] = hamming_loss(y_test3, y_predict3)\n",
    "    i += 1\n",
    "    \n",
    "# plot subset accuracy and Hamming loss\n",
    "fig, ax1 = plt.subplots(figsize=(24,12))\n",
    "ax1.set_xlabel('Folds',fontsize=20)\n",
    "ax1.set_ylabel('Subset Accuracy',fontsize=20)\n",
    "ax1.plot(np.arange(1,11), result, color='red',marker='o',markersize=10)\n",
    "ax1.legend(['Accuracy'],loc=2,fontsize=20)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel('Hamming Loss',fontsize=20)\n",
    "ax2.plot(np.arange(1,11), loss, color='blue',marker='s',markersize=10)\n",
    "ax2.legend(['Loss'],loc=1,fontsize=20)\n",
    "\n",
    "plt.title('10-fold crossvalidation, KNN',fontsize=20)\n",
    "fig.tight_layout()\n",
    "plt.grid(1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleKey = {0: \"No Title Data\", 2: \"Full Title Data\"}\n",
    "trialLoss = {v: [] for k, v in titleKey.items()}\n",
    "trialScore = {v: [] for k, v in titleKey.items()}\n",
    "for t in range(10):\n",
    "    print(\"Starting trial \", t)\n",
    "    for i, X in enumerate((shortX, fullX)):\n",
    "        title = titleKey[i]\n",
    "        X_train, X_test, y_train, y_test  = train_test_split(X, labelsY, test_size=.1)\n",
    "        RFC = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "        clf = OneVsRestClassifier(RFC)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        ham_loss = hamming_loss(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        trialScore[title].append(acc)\n",
    "        trialLoss[title].append(ham_loss)\n",
    "        \n",
    "plt.clf()\n",
    "_, ax = plt.subplots(figsize=(24,12))\n",
    "for k, v in trialScore.items():\n",
    "    ax.plot(v, label=k)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.title(\"Accuracy Of Datasets\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "_, ax = plt.subplots(figsize=(24,12))\n",
    "for k, v in trialLoss.items():\n",
    "    ax.plot(v, label=k)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.title(\"Hamming Loss Of Datasets\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
